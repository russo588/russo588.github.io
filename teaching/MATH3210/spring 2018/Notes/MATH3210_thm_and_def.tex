\documentclass[12pt,letterpaper]{article}
%\setlength{\oddsidemargin}{.0in}
%\setlength{\evensidemargin}{.0in}
%\setlength{\textwidth}{6.5in}
%\setlength{\topmargin}{-.3in}
%\setlength{\headsep}{.20in}
%\setlength{\textheight}{9.in}
\usepackage[margin=1in]{geometry}
\usepackage[leqno]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm, amscd}
\usepackage{amssymb}
\usepackage[all]{xy}
\usepackage{graphicx}
\usepackage{enumerate}


%Here are some user-defined notations
\newcommand{\RR}{\mathbf R}  %bold R
\newcommand{\CC}{\mathbf C}  %bold C
\newcommand{\ZZ}{\mathbf Z}   %bold Z
\newcommand{\QQ}{\mathbf Q}   %bold Q
\newcommand{\rr}{\mathbb R}     %blackboard bold R
\newcommand{\cc}{\mathbb C}    %blackboard bold R
\newcommand{\zz}{\mathbb Z}    %blackboard bold R
\newcommand{\qq}{\mathbb Q}   %blackboard bold Q
\newcommand{\ZZn}[1]{\ZZ/{#1}\ZZ}
\newcommand{\zzn}[1]{\zz/{#1}\zz}
\newcommand{\calM}{\mathcal M}  %calligraphic M
\newcommand{\sm}{\setminus} 
\newcommand{\bfa}{\mathbf a}
\newcommand{\bfb}{\mathbf b}
\newcommand{\bfc}{\mathbf c}


%improving spacing in tables (space above and below characters in a row)
\newcommand{\tfix}{\rule{0pt}{2.6ex}}
\newcommand{\bfix}{\rule[-1.2ex]{0pt}{0pt}}


%Here are commands with variable inputs 
\newcommand{\intf}[1]{\int_a^b{#1}\,dx}
\newcommand{\intfb}[3]{\int_{#1}^{#2}{#3}\,dx}
\newcommand{\pln}[1]{$\sm${\tt #1}}
\newcommand{\bgn}[1]{$\tt {\sm}begin\{#1\}$}
\newcommand{\nd}[1]{$\tt {\sm}end\{#1\}$}
\newcommand{\marginalfootnote}[1]{%
        \footnote{#1}
        \marginpar[\hfill{\sf\thefootnote}]{{\sf\thefootnote}}}
\newcommand{\edit}[1]{\marginalfootnote{#1}}


%Here are some user-defined operators
\newcommand{\Tr}{\operatorname {Tr}}
\newcommand{\GL}{\operatorname {GL}}
\newcommand{\SL}{\operatorname {SL}}
\newcommand{\Prob}{\operatorname {Prob}}
\newcommand{\re}{\operatorname {Re}}   %new definition of "real part" operator
\newcommand{\im}{\operatorname {Im}}   %new definition of "imaginary part" operator


%These commands deal with theorem-like environments (i.e., italic)
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}

%These deal with definition-like environments (i.e., non-italic)
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

%This numbers equations by section
\numberwithin{equation}{section}


%This is for hypertext references
\usepackage{color}
\usepackage{hyperref}


%spacing
\usepackage{setspace}




\begin{document}
\onehalfspacing
\tableofcontents
\addtocontents{toc}{~\hfill\textbf{Page}\par}

\section{Basic Notions}
\begin{definition} A binary operation on a set $S$ is a function $f:S\times S\rightarrow S$. 
\end{definition}
\begin{definition} A \emph{field} is a set $\mathbb{F}$ together with two binary operations $+$, and $\cdot$ called addition and multiplication (respectively) such that 
\ \\
\begin{enumerate}[1.]
\setlength{\itemsep}{5pt}
\item For all $a,b,c\in \mathbb{F}$ we have 
\[a+(b+c)=(a+b)+c\]
and 
\[a\cdot(b\cdot c)=(a\cdot b)\cdot c.\]
\item For all $a,b\in \mathbb{F}$ we have 
\[a+b=b+a\]
and 
\[a\cdot b=b\cdot a.\]
\item There exists an element $0\in \mathbb{F}$, called an additive identity,  such that for all $a\in \mathbb{F}$ we have $a+0=a$. 
\item There exists an element $1\in \mathbb{F}$, called a multiplicative identity, such that for all $a\in \mathbb{F}$ we have $a\cdot 1=a$.
\item For all $a\in \mathbb{F}$ there exists an element $b\in \mathbb{F}$, called an additive inverse, such that $a+b=0.$ 
\item For all $a\in \mathbb{F}$ such that $a\neq 0$ there exists an element $c\in \mathbb{F}$, called a multiplicative inverse, such that $a\cdot c=1$.
\item  For all $a,b,c\in \mathbb{F}$ 
\[a\cdot (b+c)=(a\cdot b)+ (a\cdot c)\]

\end{enumerate}
\end{definition}
\ \\
{\bf \noindent Note:} Fields have a unique additive and multiplicative identity denoted 0 and 1 respectively. Moreover, when the additive and multiplicative inverses exist they are unique.\\
\ \\
{\bf \noindent Some examples:} All of the following examples are with their standard operations. 
\begin{enumerate}[1.]
\setlength{\itemsep}{5pt}
\item $\mathbb{Q}$ (rational numbers)
\item $\mathbb{R}$ (real numbers)
\item $\mathbb{C}$ (complex numbers)
\item $\mathbb{Z}/p\mathbb{Z}$ for $p$ prime (Integers modulo p)
\end{enumerate}

{\bf\noindent Non example: }$\mathbb{Z}$ is not a field, it lacks multiplicative inverses. 

\begin{definition} A \emph{vector space} $V$ over a field $\mathbb{F}$ is a set $V$ with two operations called \emph{vector addition} and \emph{scalar multiplication} where vector addition is a function $+:V\times V\rightarrow V$ and scalar multiplication is a function $\cdot: \mathbb{F}\times V\rightarrow V$ such that 
\\
\begin{enumerate}[1.] 
\setlength{\itemsep}{5pt}
\item For all $u,v\in V$ we have 
\[u+v=v+u\]
\item For all $u,v,w\in V$ and for all $a,b\in\mathbb{F}$ we have 
\[(u+v)+w=u+(v+w)\]
and 
\[(ab)\cdot v=a\cdot(b\cdot v)\]
\item There exists a vector $0\in V$, called an additive identity, such that for all $v\in V$ we have
\[v+0=v\]

\item For all $v\in V$ we have a vector $w\in V$, called an additive inverse, such that 
\[v+w=0\]
\item For all $v\in V$ we have 
\[1\cdot v=v\]
\item For all $a,b \in \mathbb{F}$ and for all $u,v\in V$ we have
\[a\cdot(u+v)=a\cdot u+a\cdot v\] 
\end{enumerate}
\end{definition}
\ \\
{\bf \noindent Some examples:} All of the following examples are with their standard operations. 
\begin{enumerate}[1.]
\setlength{\itemsep}{5pt}
\item $\mathbb{F}^n=\left\{\begin{bmatrix}a_1\\ \vdots \\ a_n\end{bmatrix}: a_i\in \mathbb{F}\right\}$ where $\mathbb{F}$ is a field. 
\item Polynomials with coefficients in a field $\mathbb{F}$.
\item Polynomials ( with coefficients in a field $\mathbb{F}$) of degree $\leq n$
\item Continuous functions $f:X\rightarrow Y$, $C(X,Y)$, where $X$ and $Y$ are fields. 
\item Functions from a field $X$ into a field $Y$. 
\item $\mathbb{F}^\infty=\{(a_1, a_2, a_3, \ldots): a_i\in \mathbb{F}\}$. 
\end{enumerate}

\begin{proposition}Every vector space $V$ has a unique additive identity. The unique additive identity is denoted $0$. 
\end{proposition}

\begin{proposition}Every element $v\in V$ has a unique additive inverse. For all $v\in V$ its unique additive inverse is denoted $-v$. 
\end{proposition}

\begin{proposition}For all $v\in V$ we have $0\cdot v=0$. 
\end{proposition}

\begin{proposition}For all $a\in \mathbb{F}$ and $0\in V$ we have $a\cdot 0=0$.
\end{proposition}

\begin{proposition}For every $v\in V$ we have $(-1)\cdot v=-v$
\end{proposition}

\section{Basis for a Vector Space}

\begin{definition} A \emph{linear combination} of a list of vectors $v_1,\ldots, v_m$ in $V$ is a vector of the form 
\[a_1v_1+\ldots+a_mv_m\]
where $a_1,\ldots, a_m\in \mathbb{F}$. 
\end{definition}

\begin{definition} The set of all linear combinations of a list of vectors $v_1, \ldots, v_m$ in $V$ is called the \emph{span} of $v_1, \ldots,v_m$ denoted by 
$\text{span}\{v_1,\ldots,v_m\}$.
\[\text{span}\{v_1,\ldots,v_m\}=\{a_1v_1+\ldots+a_mv_m\mid a_i\in \mathbb{F}\}\]
\end{definition}

\begin{definition} If $V$ is a vector space and $V=\text{span}\{v_1,\ldots,v_m\}$ then we say that $v_1, \ldots, v_m$ span $V$. 
\end{definition}

\begin{definition} We say that a vectors space is \emph{finite dimensional} if there exists a finite list of vectors $v_1,\ldots, v_m$ such that 
\[\text{span}\{v_1,\ldots v_m\}=V\] 
Otherwise we say that $V$ is \emph{infinite dimensional}.
\end{definition}


\begin{definition} A list of vectors $v_1,\ldots, v_m$ in $V$ is called \emph{linearly independent} if the only choice of $a_1, \ldots, a_m\in \mathbb{F}$ such that 
\[a_1v_1+\ldots+a_mv_m=0\] is $a_1=a_2=\ldots =a_m$. A list is called \emph{linearly dependent} if it is not linearly independent.
\end{definition}

\begin{lemma} Suppose that $v_1,\ldots, v_m$ is a linearly dependent list in $V$. There exists a $j\in\{1,\ldots, m\}$ such that \\
\begin{enumerate}[1)]
\item $v_j\in \text{span}\{v_1,\ldots v_{j-1}\}$
\item $\text{span}\{v_1,\ldots, v_{j-1},v_j,v_{j+1},\ldots v_m\}=\text{span}\{v_1,\ldots, v_{j-1},v_{j+1},\ldots v_m\}$
\end{enumerate}
\end{lemma}

\begin{proposition} In a finite dimensional vector space the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. 
\end{proposition}


\begin{definition} A basis for a vector space $V$ is a list of vectors $\{v_1, \ldots, v_n\}$ such that 
\begin{enumerate}[1.]
\item $\{v_1, \ldots, v_n\}$ is linearly independent
\item $\text{span}\{v_1, \ldots, v_n\}=V$. 
\end{enumerate}
\end{definition}
\begin{proposition} A list of vectors $\{v_1, \ldots, v_n\}$ in $V$ is a basis for $V$ if and only if every vector $v\in V$ can be written \emph{uniquely} in the form
\[v=a_1v_1+\ldots+a_nv_n\]
for some $a_1, \ldots, a_n\in \mathbb{F}$. 
\end{proposition}
\begin{proposition} Every spanning list of vectors in $V$ can be reduced down to a basis. 
\end{proposition}

\begin{proposition} Every linearly independent list of vectors in $V$ can be extended to a basis.  
\end{proposition}

\begin{proposition} Any two basis of a finite dimensional vector space $V$ have the same length. 
\end{proposition}

\begin{definition} The dimension of a finite dimensional vector space is the length of any basis of the vector space. Denoted $\text{dim}(V)$. 
\end{definition}

\begin{proposition} Suppose $V$ is finite dimensional. Every list of linearly independent vectors whose length is equal to the dimension of $V$ is a basis. 
\end{proposition}

\begin{proposition} Suppose $V$ is finite dimensional. Every spanning list vectors whose length is equal to the dimension of $V$ is a basis. 
\end{proposition}

\section{Subspaces}
\begin{definition} A subspace of a vector space $V$ is a subset $H$ such that $H$ is a vector space under the same binary relations and field as $V$. 
\end{definition}

\begin{proposition}[Subspace Test] A subset $H$ is a subspace of $V$ if and only if 
\begin{enumerate}[1.] 
\item $0\in H$. 
\item For all $u,v\in H$ we have $u+v\in H$
\item For all $u\in H$ and $a\in \mathbb{F}$ we have $au\in H$.
\end{enumerate}
\end{proposition}

\begin{proposition} If $U$ is a subspace of a finite dimensional vector space $V$ then $\text{dim}(U)\leq\text{dim}(V)$. Moreover, $\text{dim}(U)=\text{dim}(V)$ if and only if $V=U$. 
\end{proposition}

\subsection{Direct Sums}
\begin{definition} Suppose $U_1,\ldots, U_m$ are subsets of $V$. The \emph{sum} of $U_1, \ldots, U_m$ denoted $U_1+\ldots+U_m$ is the set of all possible sums i.e., 
\[U_1+\ldots+U_m=\{u_1+\ldots+u_m\ |\ u_i\in U_i, i=1,\ldots, m\}\]
\end{definition}

\begin{proposition} If $U_1, \ldots, U_m$ are subspaces then so is $U_1+\ldots+U_m$. 
\end{proposition}

\begin{definition} Suppose $U_1,\ldots, U_m$ are subspaces of $V$. The sum $U_1+\ldots+U_m$ is a \emph{direct sum} if each element of $U_1+\ldots+U_m$ can be written in only one way as a sum $u_1+\ldots +u_m$ where $u_i\in U_i$, $i=1,\ldots,m$. The direct sum is denoted $U_1\oplus \ldots \oplus U_m$. 
\end{definition}

\begin{proposition}  $U_1+\ldots+U_m$ is a direct sum if and only if the only way to write $0$ as a sum is by taking each $u_i$ where $i=1, \ldots, m$ to be $0$. 
\end{proposition}

\begin{proposition}  The sum of two subspaces $U$ and $W$ is a direct sum if and only if\\ $U\cap W=\{0\}$. 
\end{proposition}

\begin{proposition} If $V$ is a finite dimensional vector space and $U$ is a subspace of $V$ then there exists a $W$ which is a subspace of $V$ such that $V=U\oplus W$. 
\end{proposition}

\subsection{Quotient Spaces} 

\begin{definition}Let $V$ be a vectors space and $U$ a subspace. For every $v\in V$ define 
\[v+U=\{v+u\ \mid\  u\in U\}\]
and 
\[V/U=\{v+U\ \mid\  v\in V\}\]
\end{definition}
\begin{proposition}Let $V$ be a vectorspace, $U$ a subspace and $v,w\in V$.  The following are equivalent. 
\begin{enumerate}[(a)]
\item $v-w\in U$
\item $v+U=w+U$
\item $(v+U)\cap (w+U)\neq \emptyset$
\end{enumerate}
\end{proposition}

\begin{proposition} Let $V$ be a vectorspace, $U$ a subspace, $\lambda\in \mathbb{F}$, and $v,w\in V$The set $V/U$ is a vector space with the following operations: 
\[(v+U)+(w+U)=(v+w)+U\]
\[\lambda(v+U)=(\lambda v)+U\]
\end{proposition}


\begin{proposition} Let $V$ be a finite dimensional vector space and $U$ be a subspace. 
\[\text{dim}(V/U)=\text{dim}(V)-\text{dim}(U)\]
\end{proposition}

\section{Linear Maps}
\begin{definition} A linear map from $V(\mathbb{F})$ to $W(\mathbb{F})$ is a function $T:V\rightarrow W$ such that 
\[T(\lambda x+y)=\lambda T(x)+T(y)\]
for every $x,y\in V$ and $\lambda\in \mathbb{F}$. Denote the set of all linear maps from $V$ to $W$ as $\mathcal{L}(V,W)$. 
\end{definition}
\begin{proposition} Suppose $\{v_1,\ldots, v_n\}$ is a basis of $V$ and $w_1,\ldots,w_n\in W$. There is a unique linear map $T:V\rightarrow W$ such that 
\[T(v_j)=w_j\ \quad \ j=1,\ldots, n.\]
\end{proposition}

\begin{proposition} If $T:V\rightarrow W$ is linear then 
\[T(0_V)=0_W.\]
\end{proposition}

\begin{definition} Suppose $S, T\in \mathcal{L}(V,W)$ and $\lambda \in \mathbb{F}$ define 
\[(S+T)(v)=S(v)+T(v) \text{ for all }v\in V\]
and
\[(\lambda T)(v)=\lambda T(v) \text{ for all }v\in V.\]
\end{definition}
\begin{proposition} $\mathcal{L}(V,W)$ is a vector space with the above operations.
\end{proposition}
\subsection{One to One, Onto, Invertibility, and Isomorphisms}
\begin{definition} For $T\in \mathcal{L}(V,W)$ define the \emph{null space} of $T$ (or kernel of T) to be 
\[\text{null}(T)=\{v\in V\ \mid \  Tv=0\}\subseteq V\]
and define the \emph{range} of $T$ to be 
\[\text{ran}(T)=\{Tv\ \mid \  v\in V\}\subseteq W.\]
\end{definition}

\begin{theorem} For $T\in \mathcal{L}(V,W)$ both $\text{null}(T)$ and $\text{ran}(T)$ are subspaces. A linear transformation is injective if and only if 
\[\text{null}(T)=\{0\}.\]
A linear transformation is surjective if and only if $\text{ran}(T)=W$. 
\end{theorem}
\begin{definition} The \emph{rank} of a linear transformation is the dimension of its range. The \emph{nullity} of a transformation is the dimension of its null space. 
\end{definition}

\begin{definition} A vector space isomorphism from $V$ onto $W$ is a bijective linear map $T:V\rightarrow W$. If there is a vector space isomorphism from $V$ onto $W$ we say $V$ is isomorphic to $W$ and write $V\cong W$. 
\end{definition}
\begin{theorem} Let $V$ and $W$ be two finite dimensional vector spaces. $V$ is isomorphic to $W$ if and only if $\text{dim}(V)=\text{dim}(W)$.
\end{theorem}
\begin{definition} A linear map $T:V\rightarrow W$ is \emph{invertible} if there exists an map $S:W\rightarrow V$ such that 
\[S\circ T=\text{Id}_{V}\]
\[T\circ S=\text{Id}_{W}\]
\end{definition}
\begin{theorem} A map $T:V\rightarrow W$ is invertible if and only if its bijective. 
\end{theorem}
\subsection{Fundamental Theorem of Linear Maps}
\begin{proposition} Suppose $T\in \mathcal{L}(V,W)$ and define 
\[\tilde{T}:V/\text{null}(T)\rightarrow W\] 
by
\[\tilde{T}(v+\text{null}(T))=Tv.\]
The following hold:
\begin{enumerate}[a)]
\item $\tilde{T}$ is linear
\item $\tilde{T}$ is injective
\item $\text{ran}(\tilde{T})=\text{ran}(T)$
\item $V/\text{null}(T)\cong \text{ran}(T)$.
\end{enumerate}

\end{proposition}
\begin{theorem}[Fundamental Theorem of Linear Maps/ Rank-Nullity]
Suppose $V$ is a finite dimensional vector space and $T\in \mathcal{L}(V,W)$. We have $\text{ran}(T)$ is finite dimensional and 
\[\text{dim}(V)=\text{dim}(\text{ran}(T))+\text{dim}(\text{null}(T)).\]
\end{theorem}

\begin{proposition} Suppose $V$ and $W$ are finite dimensional vector spaces. Let $T\in \mathcal{L}(V,W)$. 
\begin{enumerate}[a)]
\item If $\text{dim}(V)<\text{dim}(W)$ then $T$ is not surjective.
\item If $\text{dim}(V)>\text{dim}(W)$ then $T$ is not injective.
\end{enumerate}
\end{proposition}

\begin{definition} A linear map $T\in \mathcal{L}(V,V)$ is called an \emph{operator}. 
\end{definition}
\begin{theorem} Suppose $T$ is an operator over a vector space $V$. If $V$ is finite dimension the following are equivalent:
\begin{enumerate}[a)]
\item $T$ is injective
\item $T$ is surjective
\item $T$ is bijective
\end{enumerate}
\end{theorem}

\subsection{The Matrix of a Linear Map and the Coordinate Transform}
\begin{definition} Let $V$ be an $n$-dimensional vector space over the field $\mathbb{F}$. Let $\beta=\{v_1, \ldots, v_n\}$ be an \emph{ordered} basis for $V$. The coordinate transform $\varphi_\beta:V\rightarrow \mathbb{F}^n$ is defined by 
\[v=a_1v_1+\ldots+a_nv_n\overset{\varphi_\beta}{\longmapsto} \begin{bmatrix}a_1\\ \vdots \\ a_n\end{bmatrix}\]
Denote the column vector $\varphi_\beta(v)$ by $[v]_\beta$.
\end{definition}
\begin{proposition} The coordinate transform from $V$ onto $\mathbb{F}^n$ is a vector space isomorphism.
\end{proposition}

\begin{definition} Suppose $T:V\rightarrow W$ is a linear maps with $\beta=\{v_1,\ldots, v_n\}$ and $\gamma=\{w_1, \ldots, w_m\}$ as ordered basis for $V$ and $W$ respectively. Define the matrix of $T$, denoted by $[T]_\beta^\gamma$, by the following
If 
\[T(v_k)=a_{1,k}w_1+\ldots +a_{m,k}w_k\]
then the $k$-th column of $[T]_\beta^\gamma$ is given by $\begin{bmatrix} a_{1,k}\\ \vdots \\ a_{m,k}\end{bmatrix}$. 
\end{definition}

\begin{theorem} The following diagram commutes\\
\[
\begin{CD}
V @>T>> W\\
@V\phi_{\beta}VV @AA\phi_\gamma^{-1} A\\
\mathbb{F}^n @>[T]_{\beta}^\gamma>> \mathbb{F}^m
\end{CD}
\]
More specifically, 
\[[T]_{\beta}^\gamma[v]_{\beta}=[T(v)]_{\gamma}.\]
\end{theorem}
\section{Determinants}
\begin{definition} Let $\hat{A}_{i,j}$ be the $(n-1)\times (n-1)$ matrix that results from $A$ by removing the $i$th row and $j$th column and let $a_{i,j}$ be the entry in the $i$th row and $j$th column.  Consider the set of $n\times n$ matrices over $\mathbb{F}$.  
Define \[\det([a])=a\]
and
\[\det(A)=\sum_{i=1}^n(-1)^{i+1}a_{i,1}\det(\hat{A}_{i,1})\]
\end{definition}
\subsection{Multilinear and Alternating}\label{Multilinear and Alternating}

\begin{definition} Let $V_1, \ldots V_n$ be vector spaces over a field $\mathbb{F}$. The product $V_1\times \ldots \times V_n$ is defined by 
\[V_1\times \ldots \times V_n=\{(v_1,\ldots v_n)\ \mid \ v_1\in V_1, \ldots v_n\in V_n\}\] 
\end{definition}
Of course, with the appropriate operations $V_1\times \ldots \times V_n$ is a vector space. 
\begin{proposition} $V_1\times \ldots \times V_n$ is a vector space over $\mathbb{F}$ with the following operations:
\[(v_1,\ldots, v_n)+(u_1, \ldots, u_n)=(v_1+u_1, \ldots, v_n+u_n)\]
\[c(v_1, \ldots, v_n)=(cv_1, \ldots, cv_n).\]
\end{proposition}
The proof of the above proposition is standard, we will omit it. 
\begin{definition} Let $V_1, \ldots ,V_n, W$ be vector spaces over a field $\mathbb{F}$. 
A map $\varphi:V_1\times\ldots \times V_n\rightarrow W$ is called \emph{multilinear} if for each fixed $i$ and fixed elements $v_j\in V_j$, $j\neq i$, the map
\[V_i\rightarrow W\quad \text{ defined by }\quad x\mapsto \varphi(v_1, \ldots, v_{i-1}, x, v_{i+1}, \ldots ,v_n)\]
is linear. If each $V_i=V$, $i=1,2,\ldots n$ then $\varphi$ is called a $n$-multilinear function on $V$. If $W$ is a field, then $\varphi$ is called a multilinear form on $V$. 

The function $\det:M_{n\times n}(\mathbb{F})\rightarrow \mathbb{F}$ is viewed as a multilinear map by viewing the columns of a matrix as column vectors and making the following identification. 
\[M_{n\times n}(\mathbb{F})\ni A=[v_1,\ldots, v_n]\mapsto (v_1, \ldots, v_n)\in \mathbb{F}^n\times \ldots \times \mathbb{F}^n.\]
\end{definition}

\begin{proposition}$\det:M_{n\times n}(\mathbb{F})\rightarrow \mathbb{F}$ is a multilinear function (viewing each matrix as a tuple of column vectors in $\mathbb{F}^n\times \ldots \times \mathbb{F}^n$).
\end{proposition}

\begin{definition}An $n$-multilinear function $\varphi$ on $V$ is called alternating if $\varphi$ is zero whenever two consecutive arguments are equal, i.e. if $v_i=v_{i+1}$ for some $i\in\{1,\ldots, n-1\}$, then $\varphi(v_1, \ldots ,v_n)=0$. 
\end{definition}


\begin{lemma}Let $B\in M_{n\times n}(\mathbb{F})$, where $n\geq 2$. If the column $j$ of $B$ equals 
\[e_k=\left[\begin{array}{c}0\\\vdots\\ 1\\\vdots\\ 0\end{array}\right] - \text{ k}\small{th} \text{ spot}\] for some $k$ $(1\leq k\leq n)$, then 
\[\det(B)=(-1)^{j+k}\det(\hat{B}_{k,j})\]
\end{lemma}

\begin{theorem} The determinant of a square matrix can be evaluated by cofactor expansion along any column or row, i.e. 
\[\det(A)=\sum_{i=1}^n (-1)^{i+j}a_{i,j} \det(\hat{A}_{i,j})\]
or 
\[\det(A)=\sum_{j=1}^n (-1)^{i+j}a_{i,j} \det(\hat{A}_{i,j})\]
\end{theorem}


\begin{proposition}
The determinant function $\det:M_{n\times n}(\mathbb{F})\rightarrow \mathbb{F}$ is an alternating function.
\end{proposition}

\begin{theorem}\label{unique}
The determinant $\det:M_{n\times n}(\mathbb{F})\rightarrow \mathbb{F}$ is the \emph{unique} multilinear alternating map taking the identity matrix to the mulitiplicative identity element in $\mathbb{F}$
\end{theorem}


\subsection{The Multiplicative Property}


\begin{definition}[Elementary Row Operations]
An elementary row operation is any one of the following operations performed on a matrix. 
\begin{enumerate}[$\bullet$]
\item Switching the position of two rows.
\item Multiplying the entries of a row by a scalar.
\item Replacing a row with its addition of a scalar multiple of another row. 
\end{enumerate}
\end{definition} 
\begin{lemma}\label{ele prod}
Let $A$ and $B$ be matrices such that $C=AB$ is defined. Suppose $e_1, \ldots e_n$ be a sequence of elementary row operations. Let $A^\prime$ be the matrix resulting from performing $e_1,\ldots, e_n$ on $A$ and $C^\prime$ be the matrix resulting from performing $e_1, \ldots, e_n$ on $C$. Then 
\[C^\prime=A^\prime B\]
\end{lemma}

\begin{lemma}\label{det ele}
 Suppose $e_1, \ldots e_n$ be a sequence of elementary row operations. Let $A^\prime$ be the matrix resulting from performing $e_1,\ldots, e_n$ on $A$. We have that 
 \[\alpha\det(A^\prime)= \det{A}\]
 for some $\alpha\in \mathbb{F}$ depending only on $e_1,\ldots, e_n$. 
\end{lemma}

\begin{definition}
Let $A$ be a matrix. Define its transpose, denoted $A^\top$, by
\[(A^\top)_{i,j}=A_{j,i}\]
\end{definition}

\begin{lemma}
Let $A$ and $B$ be matrices such that $AB$ is defined. 
\[(AB)^\top=B^\top A^\top\]
\end{lemma}

\begin{lemma}\label{det trans}
For any square matrix $A$ 
\[\det(A)=\det(A^\top)\]
\end{lemma}


\begin{definition} A lower triangular matrix is any matrix $L$ such that 
\[L_{i,j}=0 \quad\text{ for }\quad j>i\]
\end{definition}

\begin{definition} An upper triangular matrix is any matrix $U$ such that 
\[U_{i,j}=0 \quad\text{ for }\quad j<i\]
\end{definition}

\begin{lemma} The product of upper (lower) triangular matrices is an upper (lower) triangular matrix.
\end{lemma}

\begin{lemma}\label{det tri}
Let $A$ be either an upper triangular matrix or lower triangular matrix. We have that 
\[\det(A)=\prod a_{i,i}.\]
\end{lemma}


\begin{proposition}\label{multiplicativeid}
\[\det(AB)=\det(A)\det(B)\]
\end{proposition}

\subsection{Invertibility of a Matrix}
\begin{definition}A matrix $A\in M_{n\times n}(\mathbb{F})$ is said to be invertible if there exists a matrix $B\in M_{n\times n}(\mathbb{F})$ such that 
\[AB=I\]
\[BA=I\]
where $I$ is the $n\times n$ identity matrix. 
\end{definition}

\begin{definition}
Suppose $S$ is a set with a binary operation $\cdot :S\times S\rightarrow S$, then $S$ with $\cdot$ is a monoid if 
\begin{enumerate}[(a)]
\item For all $a, b$ and $c$ in $S$ 
\[(a\cdot b)\cdot c=a\cdot (b\cdot c)\]
\item There exists an identity element $1$ in $S$ such that for all $a\in S$
\[a\cdot 1=1\cdot a=a.\]
\end{enumerate}
\end{definition}

\begin{definition}
Let $A$ be a monoid and $1$ its identity element. We say $a\in A$ is invertible if and only if there exists a $b\in A$ such that
\[a\cdot b=b\cdot a=1\]
\end{definition}

\begin{definition}
A monoid homomorphism between monoids $A$ and $B$ is a function $\phi:A\rightarrow B$ such that 
\[\phi(a_1)\cdot \phi(a_2)=\phi(a_1\cdot a_2)\]
for all $a_1, a_2\in A$ 
and \[\phi(1_A)=1_B\] for the identity elements $1_A\in A$ and $1_B\in B$.
\end{definition}

\begin{theorem}
Let $A$ and $B$ be two monoids and $\phi:A\rightarrow B$ a monoid homomorphism. If $a\in A$ is invertible then $\phi(a)$ is invertible. 
\end{theorem}


\begin{theorem}\label{matinv}
Let $T\in \mathcal{L}(V,W)$ and let $\beta=\{v_1,\ldots v_n\}$, $\gamma=\{w_1, \ldots w_m\}$ be bases for $V$ and $W$ respectively. The transformation $T$ is invertible if and only if $[T]_\beta^\gamma$ is invertible.
\end{theorem}

\begin{definition}
An elementary matrix is any matrix obtained from performing a single row operation on the identity matrix. 
\end{definition}


\begin{proposition} Let $A$ be a matrix and let $B$ be the matrix obtained from $A$ by performing a row operation with corresponding elementary matrix $E$. We have 
\[EA=B\]
\end{proposition}


\begin{proposition}If $A$ is a matrix and $B$ is an upper triangular matrix obtained from $A$ via a finite sequence of row operations, then $A$ is invertible if and only if $B$ is invertible. Moreover, $B$ is invertible if and only if the entries on the diagonal are non-zero. 
\end{proposition}


\begin{theorem} A matrix $A$ is invertible if and only if $\det(A)\neq 0$. 
\end{theorem}

\subsection{Properties and Facts}
\begin{enumerate}[$\bullet$]
\item If $A$ is a triangular matrix then $\det(A)$ is the product of the entries on the main diagonal.
\item If a multiple of one row of $A$ is added to another row to produce $B$ then $\det(A)=\det(B)$.
\item If two rows of $A$ are interchanged to produce $B$ then $\det(A)=-\det(B)$. 
\item If one row of $A$ is multiplied by $k$ to produce $B$ then $\det(B)=k\cdot \det(A)$. 
\item A square matrix is invertible if and only if $\det(A)\neq 0$.
\item $\det(A^\top)=\det(A)$. 
\item $\det(AB)=\det(A)\det(B)$.
\item $\det(I)=1$.
\end{enumerate}
\section{Eigenvalues and Eigenvectors}
\begin{definition} Suppose $T\in \mathcal{L}(V)$. A subspace $U$ of $V$ is called \emph{invariant under} $T$ if $u\in U$ implies that $Tu\in U$. 
\end{definition}
\begin{definition}Suppose $T\in \mathcal{L}(V)$. A scalar $\lambda\in \mathbb{F}$ is an \emph{eigenvalue} of $T$ if there exists a $v\in V$ such that $v\neq 0$ and $Tv=\lambda v$. The vector $v$ is called an \emph{eigenvector}.
\end{definition}
\begin{proposition}$T\in \mathcal{L}(V)$ has a one dimensional subspace if and only if $T$ has an eigenvalue. 
\end{proposition}

\begin{theorem}Suppose $V$ is a finite dimensional vector space and $T\in \mathcal{L}(V)$. The following are equivalent:
\begin{enumerate}[(a)]
\item $\lambda\in \mathbb{F}$ is an eigenvalue.
\item $T-\lambda I$ is not injective.
\item $T-\lambda I$ is not surjective.
\item $T-\lambda I$ is not invertible.
\end{enumerate}
\end{theorem}

\begin{proposition} Suppose  $T\in \mathcal{L}(V)$. If $\lambda_1, \ldots, \lambda_m$ are distinct eigenvalues of $T$ and $v_1, \ldots, v_m$ are corresponding eigenvectors then $\{v_1, \ldots, v_m\}$ is a linearly independent set. 
\end{proposition}

\begin{definition}Suppose  $T\in \mathcal{L}(V)$. Define for positive $m$
\[T^m=\underbrace{T\circ\ldots\circ T}_{m\text{ times}}\]
\[T^0=Id\]
and if $T$ is invertible
\[T^{-m}=\underbrace{T^{-1}\circ\ldots\circ T^{-1}}_{m\text{ times}}\]
\end{definition}

\begin{definition}Suppose  $T\in \mathcal{L}(V)$ and let $p(x)=a_nx^n+\ldots+a_1x+a_0$ be a polynomial over $\mathbb{F}$. Define
\[p(T)=a_nT^n+\ldots+a_1T+a_0Id\]

\end{definition}


\end{document}








